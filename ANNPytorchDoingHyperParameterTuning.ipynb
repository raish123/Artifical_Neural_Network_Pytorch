{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d27285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #main frame work we used to perform high dimension tensor calculation\n",
    "import torch.nn as nn #this class we used to define Custom Neural Network Architecture\n",
    "from torch.utils.data import Dataset,DataLoader #this Dataset class we used split the entire data into chunks based on Batch_sizes\n",
    "#where as DataLoader class we used to load data and perform action over it.\n",
    "\n",
    "from sklearn.model_selection import train_test_split #this class we used to split the data into train or test sets\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler \n",
    "#above this class used to do input scaling so that variance of all field would be same for models\n",
    "\n",
    "import pandas as pd\n",
    "from cloudpickle import pickle #this class we used to convert datastructure into bytes stream\n",
    "import seaborn as sb #to see the visualization.\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51dd95b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if i want to show gpu is avilable or not in system.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3f295",
   "metadata": {},
   "source": [
    "### Now Loading Dataset and doing EDA over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0344507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             1338 non-null   int64  \n",
      " 1   sex             1338 non-null   int64  \n",
      " 2   bmi             1338 non-null   float64\n",
      " 3   children        1338 non-null   int64  \n",
      " 4   smoker          1338 non-null   int64  \n",
      " 5   region          1338 non-null   int64  \n",
      " 6   charges         1338 non-null   float64\n",
      " 7   insuranceclaim  1338 non-null   int64  \n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 83.8 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(filepath_or_buffer=\"insurance.csv\",encoding=\"utf-8\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689c879",
   "metadata": {},
   "source": [
    "### step:1 checking for Null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f38134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0.0\n",
       "sex               0.0\n",
       "bmi               0.0\n",
       "children          0.0\n",
       "smoker            0.0\n",
       "region            0.0\n",
       "charges           0.0\n",
       "insuranceclaim    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * data.isnull().sum()/data.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a41dfd",
   "metadata": {},
   "source": [
    "### step:2 checking for duplicated record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc94ecdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392bef2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>insuranceclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>30.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1639.5631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex    bmi  children  smoker  region    charges  insuranceclaim\n",
       "581   19    1  30.59         0       0       1  1639.5631               1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see which records showing duplicacy\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ba9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping those duplocate records from data.\n",
    "data.drop_duplicates(keep=\"first\",inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d43739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#again checking duplicated record deleted or not.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ae36f",
   "metadata": {},
   "source": [
    "### step:3 checking for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da4d896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insuranceclaim\n",
       "1    0.584892\n",
       "0    0.415108\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.insuranceclaim.value_counts(normalize=True,ascending=False,dropna=False) #both the classes are balanced in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2fefd",
   "metadata": {},
   "source": [
    "### step:4 splitting the data into training set or testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc19223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting input and output variable\n",
    "x = data.drop(labels=\"insuranceclaim\",axis=1)\n",
    "y = data[\"insuranceclaim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d00516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1069, 7), (268, 7), (1069,), (268,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42,stratify=y) \n",
    "#stratify ensure both class distribution goes same to both sets(train or test)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6feb8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimension: 2 and Testing set dimension: 2\n",
      "Training set dtype:   age           int64\n",
      "sex           int64\n",
      "bmi         float64\n",
      "children      int64\n",
      "smoker        int64\n",
      "region        int64\n",
      "charges     float64\n",
      "dtype: object and Testing set dtype: age           int64\n",
      "sex           int64\n",
      "bmi         float64\n",
      "children      int64\n",
      "smoker        int64\n",
      "region        int64\n",
      "charges     float64\n",
      "dtype: object\n",
      "Training set type:   <class 'pandas.core.frame.DataFrame'> and Testing set type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#to show the dtype and dimension of train test split data\n",
    "print(f\"Training set dimension: {x_train.ndim} and Testing set dimension: {x_test.ndim}\")\n",
    "print(f\"Training set dtype:   {x_train.dtypes} and Testing set dtype: {x_test.dtypes}\")\n",
    "print(f\"Training set type:   {type(x_train)} and Testing set type: {type(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2ac28",
   "metadata": {},
   "source": [
    "```\n",
    "| Task Type                                | Input dtype     | Target dtype    | Loss Function          |\n",
    "| ---------------------------------------- | --------------- | --------------- | ---------------------- |\n",
    "| **Binary Classification (Sigmoid)**      | `torch.float32` | `torch.float32` | `nn.BCEWithLogitsLoss` |\n",
    "| **Multi-class Classification (Softmax)** | `torch.float32` | `torch.long`    | `nn.CrossEntropyLoss`  |\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b3500e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>34.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>34828.65400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>20.790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1607.51010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>23.465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3206.49135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>33.660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1136.39940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>35.200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4670.64000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex     bmi  children  smoker  region      charges\n",
       "1290   19    1  34.900         0       1       3  34828.65400\n",
       "359    18    0  20.790         0       0       2   1607.51010\n",
       "579    25    0  23.465         0       0       0   3206.49135\n",
       "662    18    1  33.660         0       0       2   1136.39940\n",
       "1167   32    1  35.200         2       0       3   4670.64000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0213174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1290    1\n",
       "359     0\n",
       "579     0\n",
       "662     1\n",
       "1167    0\n",
       "Name: insuranceclaim, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892bb57",
   "metadata": {},
   "source": [
    "### step:5 now doing input scaling converting into numpy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a32a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimension: 2  and Testing set dimension: 2\n",
      "Training set dtype:     float32 and Testing set dtype: float32\n",
      "Training set type:      <class 'numpy.ndarray'> and Testing set type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#creating an object of standardscaler class.\n",
    "ss = StandardScaler()\n",
    "\n",
    "#now applying scaling on input data.\n",
    "x_train_ss = ss.fit_transform(x_train.astype(\"float32\")) #converting dataframe 2d object to 2d numpy object\n",
    "x_test_ss  = ss.transform(x_test.astype(\"float32\"))      #Transforming testing dataframe 2d object to 2d numpy object\n",
    "print(f\"Training set dimension: {x_train_ss.ndim}  and Testing set dimension: {x_test_ss.ndim}\")\n",
    "print(f\"Training set dtype:     {x_train_ss.dtype} and Testing set dtype: {x_test_ss.dtype}\")\n",
    "print(f\"Training set type:      {type(x_train_ss)} and Testing set type: {type(x_test_ss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a19c775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimension: 1 and Testing set dimension: 1\n",
      "Training set type:   <class 'numpy.ndarray'> and Testing set type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#converting target variable set to numpy object.\n",
    "import numpy as np\n",
    "y_train_np = np.array(object=y_train,dtype=np.float32)\n",
    "y_test_np = np.array(object=y_test,dtype=np.float32)\n",
    "print(f\"Training set dimension: {y_train_np.ndim} and Testing set dimension: {y_test_np.ndim}\")\n",
    "print(f\"Training set type:   {type(y_train_np)} and Testing set type: {type(y_test_np)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ada41e",
   "metadata": {},
   "source": [
    "### step:6 converting numpy object to tensor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6165d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set tensor: <class 'torch.Tensor'> and Testing set tensor: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train_ss)\n",
    "x_test_tensor = torch.from_numpy(x_test_ss)\n",
    "y_train_tensor = torch.from_numpy(y_train_np)\n",
    "y_test_tensor = torch.from_numpy(y_test_np)\n",
    "print(f\"Training set tensor: {type(x_train_tensor)} and Testing set tensor: {type(x_test_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5da2d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4381,  0.9879,  0.7013,  ...,  2.0106,  1.3720,  1.8369],\n",
       "         [-1.5094, -1.0122, -1.6309,  ..., -0.4974,  0.4717, -0.9687],\n",
       "         [-1.0101, -1.0122, -1.1888,  ..., -0.4974, -1.3291, -0.8337],\n",
       "         ...,\n",
       "         [-0.8675,  0.9879, -0.2491,  ...,  2.0106,  0.4717,  0.4365],\n",
       "         [ 1.0582, -1.0122,  0.3343,  ..., -0.4974, -1.3291, -0.1819],\n",
       "         [ 0.3450, -1.0122,  0.2781,  ..., -0.4974,  0.4717, -0.4598]]),\n",
       " tensor([1., 0., 0.,  ..., 1., 1., 1.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor,y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c68608",
   "metadata": {},
   "source": [
    "### step:7) using Custom Dataset class to split the dataset into chunks based on Batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb486c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    #using constructor class to initalize the instance variable init\n",
    "    def __init__(self,input,output):\n",
    "        self.input  = input\n",
    "        self.output = output\n",
    "        \n",
    "    #using another magical method to get shape of input data.\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    #using another magical method to split data into chunks.\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index],self.output[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "044cfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an object of CustomDataset class.\n",
    "train_dataset = CustomDataset(input=x_train_tensor,output=y_train_tensor)\n",
    "test_dataset  = CustomDataset(input=x_test_tensor,output=y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08b48e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1069, 268)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aeb2b9",
   "metadata": {},
   "source": [
    "### dataloader class we used to load the data based on batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984e86c",
   "metadata": {},
   "source": [
    "### step:8 Optuna Hyperparameter at Backened smartly they will used bayessian search technique to find value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "925ef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "#now defining deep neural network Architecture\n",
    "class SimpleNeuralArchitecture(nn.Module):\n",
    "    \n",
    "    #using constructor method to define neural network architecture variable init.\n",
    "    def __init__(self,input_dim,output_dim,num_hidden_layer,neurons_per_hidden_layer,dropout_rate):\n",
    "        \n",
    "        #inheriting parent class constructor.\n",
    "        super().__init__()\n",
    "    \n",
    "        \n",
    "        layers = [] #whole neural architecture layer we are storing Here\n",
    "        \n",
    "        for i in range(num_hidden_layer):\n",
    "            # adding a fully connected (Linear) layer\n",
    "            layers.append(nn.Linear(in_features=input_dim,out_features=neurons_per_hidden_layer))\n",
    "            \n",
    "            # BatchNorm\n",
    "            layers.append(nn.BatchNorm1d(neurons_per_hidden_layer))\n",
    "            \n",
    "            # Activation function using Relu \n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout\n",
    "            layers.append(nn.Dropout(p=dropout_rate))\n",
    "            \n",
    "            # updating input dimension for the next layer\n",
    "            input_dim = neurons_per_hidden_layer\n",
    "        \n",
    "        \n",
    "        # Finally, adding the output layer\n",
    "        layers.append(nn.Linear(in_features=input_dim, out_features=output_dim))\n",
    "        \n",
    "        \n",
    "        # Combine all layers into a Sequential block\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        #calling the weight initialization method.\n",
    "        self._init_weights()\n",
    "        \n",
    "        \n",
    "    # Weight initialization\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            #Yaani, self.modules() ek PyTorch built-in method hai \n",
    "            #jo tumhare nn.Module (yani neural network class) ke andar ke saare submodules return karta hai.\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # ReLU ke liye He initialization\n",
    "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                init.zeros_(m.bias)\n",
    "        \n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        x = self.model(input_data)\n",
    "        # output ke liye sigmoid lagana hai agar binary classification ho\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71ec9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the objective function\n",
    "def objective(trial):\n",
    "    #Define the Search Space in ANN(like nos of hidden layer,nos of neurons per layer...so on)\n",
    "    num_hidden_layer = trial.suggest_int(\"num_hidden_layer\", 1, 6)\n",
    "    neurons_per_hidden_layer = trial.suggest_int(\"neurons_per_hidden_layer\", 16, 256, step=16)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 60,step=10)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    \n",
    "    #defining the model and its all parameter\n",
    "    input_dim = x_train_tensor.shape[1]\n",
    "    output_dim = 1\n",
    "    \n",
    "    \n",
    "    #creating an object of SimpleNeuralArchitecture class\n",
    "    model = SimpleNeuralArchitecture(input_dim=input_dim,output_dim=output_dim,\n",
    "                                     num_hidden_layer=num_hidden_layer,\n",
    "                                     neurons_per_hidden_layer=neurons_per_hidden_layer,\n",
    "                                     dropout_rate = dropout_rate\n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                     \n",
    "                                     ).to(device=device)\n",
    "\n",
    "    \n",
    "    #now initializing the remaining model parameter like epoch optimizer ...so on\n",
    "    \n",
    "    #target are binary classifier so loss function binary cross entropy.\n",
    "    loss_fxn = nn.BCELoss() #calculate loss value\n",
    "\n",
    "    \n",
    "    \n",
    "    if optimizer_name == \"Adam\":\n",
    "        #optimizers i am using Adam they will updates the trainable parameter effectively in each layer.\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(),lr=lr,weight_decay=weight_decay) \n",
    "    \n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        #optimizers i am using Adam they will updates the trainable parameter effectively in each layer.\n",
    "        optimizer = torch.optim.RMSprop(params=model.parameters(),lr=lr,weight_decay=weight_decay) \n",
    "      \n",
    "    \n",
    "    elif optimizer_name == \"SGD\":\n",
    "        #optimizers i am using Adam they will updates the trainable parameter effectively in each layer.\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(),lr=lr,weight_decay=weight_decay) \n",
    "    \n",
    "    # EarlyStopping variables\n",
    "    best_loss = np.inf         # abhi tak ka sabse best (minimum) test loss\n",
    "    patience_counter = 0       # kitne epochs ho gaye bina improvement ke\n",
    "    patience = 5               # agar 5 epoch tak test loss improve nahi hota to stop\n",
    "    \n",
    "    \n",
    "    \n",
    "    training_dataloader   = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "    testing_dataloader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
    "    \n",
    "    \n",
    "    #Now training and evaluating the performance of model\n",
    "    training_loss_lst = []\n",
    "    testing_loss_lst = []\n",
    "    for i in range(epochs):\n",
    "        #performing model training\n",
    "        model.train()\n",
    "        training_running_loss_count = 0\n",
    "        \n",
    "        #based on batchsize data record we are passing to neural network architecture.\n",
    "        for feature,label in training_dataloader:\n",
    "            feature = feature.to(device)\n",
    "            label   = label.to(device)\n",
    "            \n",
    "            #whenever we start training the model first algorithm work is forward propogation\n",
    "            pred_output = model.forward(feature)\n",
    "            #print(f\"actual output shape: {label.shape}\")\n",
    "            #print(f\"predicted output shape: {pred_output.shape}\")\n",
    "            \n",
    "            #calculating the loss value by using Loss fxns or evaluating the performance of model at time of training.\n",
    "            loss_value = loss_fxn(pred_output.squeeze(1),label)\n",
    "            \n",
    "            #now updating the training_running_loss_count variable\n",
    "            training_running_loss_count = training_running_loss_count+loss_value.item() \n",
    "            #this item method we used to get value from shape of tensor\n",
    "            \n",
    "            #before applying backpropogation to the loss value first we have to clear the gradient inside optimizers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #reducing the loss value by using backpropogation algorithm and calcualting gradient of loss wrt to trainable parameter\n",
    "            loss_value.backward()\n",
    "            \n",
    "            #using optimizers updating the tainable parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_training_loss = (training_running_loss_count)/(len(training_dataloader))\n",
    "        training_loss_lst.append(avg_training_loss)\n",
    "\n",
    "        \n",
    "        #performing model testing(on testing we dont apply any backpropogation,overfitting technique,early stopping)\n",
    "        model.eval()\n",
    "        testing_running_loss_count = 0\n",
    "        \n",
    "        with torch.no_grad(): #at time of testing or inferencing gradient tracking is off not applying overfitting technique too\n",
    "            #based on batchsize data record we are passing to neural network architecture.\n",
    "            for feature,label in testing_dataloader:\n",
    "                feature = feature.to(device)\n",
    "                label   = label.to(device)\n",
    "                \n",
    "                #whenever we start training the model first algorithm work is forward propogation\n",
    "                pred_output = model.forward(feature)\n",
    "                \n",
    "                #calculating the loss value by using Loss fxns or evaluating the performance of model at time of training.\n",
    "                loss_value = loss_fxn(pred_output.squeeze(1),label)\n",
    "                \n",
    "                #updating the count of loss in testing_running_loss_count\n",
    "                testing_running_loss_count = testing_running_loss_count+loss_value.item()\n",
    "                \n",
    "            avg_testing_loss = (testing_running_loss_count)/(len(testing_dataloader))\n",
    "            testing_loss_lst.append(avg_testing_loss)\n",
    "                \n",
    "        print(f\"Epoch [{i+1}/{epochs}], \"\n",
    "            f\"Train Loss: {avg_training_loss:.4f}, \"\n",
    "            f\"Test Loss: {avg_testing_loss:.4f}, \"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # ---------------- EARLY STOPPING + CHECKPOINT ----------------\n",
    "        if avg_testing_loss < best_loss:\n",
    "            # Agar test loss improve ho gaya hai\n",
    "            best_loss = avg_testing_loss\n",
    "            patience_counter = 0  # reset patience\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")  # ModelCheckpoint\n",
    "            print(f\"✅ Model improved. Saved at epoch {i+1} with Test Loss {best_loss:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            # Agar test loss improve nahi hua\n",
    "            patience_counter += 1\n",
    "            print(f\"⚠️ No improvement. Patience counter = {patience_counter}/{patience}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"⏹ Early stopping at epoch {i+1} (best loss = {best_loss:.4f})\")\n",
    "                break\n",
    "            \n",
    "    return best_loss  # <-- Very important for Optuna\n",
    "    print(\"Training Finished ✅\")   \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4cf0d",
   "metadata": {},
   "source": [
    "# creating study object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa0462cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:43:32,832] A new study created in memory with name: no-name-7949942d-8ca4-48d5-a0a7-5841a12edc61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<optuna.study.study.Study at 0x1fe3afce770>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52dbf674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.7049, Test Loss: 0.5366, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.5366\n",
      "Epoch [2/20], Train Loss: 0.5850, Test Loss: 0.4657, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.4657\n",
      "Epoch [3/20], Train Loss: 0.5112, Test Loss: 0.4077, \n",
      "✅ Model improved. Saved at epoch 3 with Test Loss 0.4077\n",
      "Epoch [4/20], Train Loss: 0.4610, Test Loss: 0.3755, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.3755\n",
      "Epoch [5/20], Train Loss: 0.4415, Test Loss: 0.3623, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.3623\n",
      "Epoch [6/20], Train Loss: 0.4271, Test Loss: 0.3623, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.3623\n",
      "Epoch [7/20], Train Loss: 0.4143, Test Loss: 0.3506, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.3506\n",
      "Epoch [8/20], Train Loss: 0.3795, Test Loss: 0.3569, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [9/20], Train Loss: 0.3887, Test Loss: 0.3271, \n",
      "✅ Model improved. Saved at epoch 9 with Test Loss 0.3271\n",
      "Epoch [10/20], Train Loss: 0.3774, Test Loss: 0.3217, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.3217\n",
      "Epoch [11/20], Train Loss: 0.3871, Test Loss: 0.3337, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [12/20], Train Loss: 0.3767, Test Loss: 0.3221, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [13/20], Train Loss: 0.3647, Test Loss: 0.3160, \n",
      "✅ Model improved. Saved at epoch 13 with Test Loss 0.3160\n",
      "Epoch [14/20], Train Loss: 0.3942, Test Loss: 0.3174, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [15/20], Train Loss: 0.3737, Test Loss: 0.3353, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [16/20], Train Loss: 0.3670, Test Loss: 0.3185, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [17/20], Train Loss: 0.3554, Test Loss: 0.3227, \n",
      "⚠️ No improvement. Patience counter = 4/5\n",
      "Epoch [18/20], Train Loss: 0.3494, Test Loss: 0.3114, \n",
      "✅ Model improved. Saved at epoch 18 with Test Loss 0.3114\n",
      "Epoch [19/20], Train Loss: 0.3678, Test Loss: 0.3106, \n",
      "✅ Model improved. Saved at epoch 19 with Test Loss 0.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:43:44,095] Trial 0 finished with value: 0.3105880320072174 and parameters: {'num_hidden_layer': 4, 'neurons_per_hidden_layer': 16, 'lr': 0.0039235629605612376, 'weight_decay': 6.80998035096758e-05, 'dropout_rate': 0.2164409529939766, 'batch_size': 16, 'epochs': 20, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.3105880320072174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Train Loss: 0.3965, Test Loss: 0.3164, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [1/10], Train Loss: 0.8449, Test Loss: 0.7696, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.7696\n",
      "Epoch [2/10], Train Loss: 0.8533, Test Loss: 0.7506, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.7506\n",
      "Epoch [3/10], Train Loss: 0.8153, Test Loss: 0.7382, \n",
      "✅ Model improved. Saved at epoch 3 with Test Loss 0.7382\n",
      "Epoch [4/10], Train Loss: 0.8134, Test Loss: 0.7288, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.7288\n",
      "Epoch [5/10], Train Loss: 0.7957, Test Loss: 0.6983, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.6983\n",
      "Epoch [6/10], Train Loss: 0.7812, Test Loss: 0.7188, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [7/10], Train Loss: 0.7542, Test Loss: 0.7018, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [8/10], Train Loss: 0.7532, Test Loss: 0.6757, \n",
      "✅ Model improved. Saved at epoch 8 with Test Loss 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:43:45,149] Trial 1 finished with value: 0.6756561928325229 and parameters: {'num_hidden_layer': 1, 'neurons_per_hidden_layer': 48, 'lr': 0.00062580887205395, 'weight_decay': 0.0013627574583458893, 'dropout_rate': 0.30466844459148124, 'batch_size': 32, 'epochs': 10, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.3105880320072174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.7267, Test Loss: 0.6769, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [10/10], Train Loss: 0.7273, Test Loss: 0.6882, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [1/50], Train Loss: 0.8963, Test Loss: 0.7136, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.7136\n",
      "Epoch [2/50], Train Loss: 0.8229, Test Loss: 0.6765, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.6765\n",
      "Epoch [3/50], Train Loss: 0.7777, Test Loss: 0.6276, \n",
      "✅ Model improved. Saved at epoch 3 with Test Loss 0.6276\n",
      "Epoch [4/50], Train Loss: 0.7497, Test Loss: 0.6111, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.6111\n",
      "Epoch [5/50], Train Loss: 0.7084, Test Loss: 0.5902, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.5902\n",
      "Epoch [6/50], Train Loss: 0.6797, Test Loss: 0.5855, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.5855\n",
      "Epoch [7/50], Train Loss: 0.6650, Test Loss: 0.5555, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.5555\n",
      "Epoch [8/50], Train Loss: 0.6243, Test Loss: 0.5402, \n",
      "✅ Model improved. Saved at epoch 8 with Test Loss 0.5402\n",
      "Epoch [9/50], Train Loss: 0.6219, Test Loss: 0.5218, \n",
      "✅ Model improved. Saved at epoch 9 with Test Loss 0.5218\n",
      "Epoch [10/50], Train Loss: 0.6048, Test Loss: 0.5073, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.5073\n",
      "Epoch [11/50], Train Loss: 0.5958, Test Loss: 0.5247, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [12/50], Train Loss: 0.5604, Test Loss: 0.5235, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [13/50], Train Loss: 0.5498, Test Loss: 0.4956, \n",
      "✅ Model improved. Saved at epoch 13 with Test Loss 0.4956\n",
      "Epoch [14/50], Train Loss: 0.5543, Test Loss: 0.4767, \n",
      "✅ Model improved. Saved at epoch 14 with Test Loss 0.4767\n",
      "Epoch [15/50], Train Loss: 0.5452, Test Loss: 0.4637, \n",
      "✅ Model improved. Saved at epoch 15 with Test Loss 0.4637\n",
      "Epoch [16/50], Train Loss: 0.5345, Test Loss: 0.4594, \n",
      "✅ Model improved. Saved at epoch 16 with Test Loss 0.4594\n",
      "Epoch [17/50], Train Loss: 0.5070, Test Loss: 0.4619, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [18/50], Train Loss: 0.5069, Test Loss: 0.4652, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [19/50], Train Loss: 0.5341, Test Loss: 0.4561, \n",
      "✅ Model improved. Saved at epoch 19 with Test Loss 0.4561\n",
      "Epoch [20/50], Train Loss: 0.5106, Test Loss: 0.4519, \n",
      "✅ Model improved. Saved at epoch 20 with Test Loss 0.4519\n",
      "Epoch [21/50], Train Loss: 0.5002, Test Loss: 0.4342, \n",
      "✅ Model improved. Saved at epoch 21 with Test Loss 0.4342\n",
      "Epoch [22/50], Train Loss: 0.4802, Test Loss: 0.4503, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [23/50], Train Loss: 0.5064, Test Loss: 0.4251, \n",
      "✅ Model improved. Saved at epoch 23 with Test Loss 0.4251\n",
      "Epoch [24/50], Train Loss: 0.4690, Test Loss: 0.4425, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [25/50], Train Loss: 0.4884, Test Loss: 0.4318, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [26/50], Train Loss: 0.4569, Test Loss: 0.4249, \n",
      "✅ Model improved. Saved at epoch 26 with Test Loss 0.4249\n",
      "Epoch [27/50], Train Loss: 0.4755, Test Loss: 0.4176, \n",
      "✅ Model improved. Saved at epoch 27 with Test Loss 0.4176\n",
      "Epoch [28/50], Train Loss: 0.4672, Test Loss: 0.4189, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [29/50], Train Loss: 0.4528, Test Loss: 0.4194, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [30/50], Train Loss: 0.4636, Test Loss: 0.4177, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [31/50], Train Loss: 0.4778, Test Loss: 0.4044, \n",
      "✅ Model improved. Saved at epoch 31 with Test Loss 0.4044\n",
      "Epoch [32/50], Train Loss: 0.4562, Test Loss: 0.4186, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [33/50], Train Loss: 0.4464, Test Loss: 0.4056, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [34/50], Train Loss: 0.4847, Test Loss: 0.4226, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [35/50], Train Loss: 0.4792, Test Loss: 0.3927, \n",
      "✅ Model improved. Saved at epoch 35 with Test Loss 0.3927\n",
      "Epoch [36/50], Train Loss: 0.4542, Test Loss: 0.4038, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [37/50], Train Loss: 0.4616, Test Loss: 0.3918, \n",
      "✅ Model improved. Saved at epoch 37 with Test Loss 0.3918\n",
      "Epoch [38/50], Train Loss: 0.4353, Test Loss: 0.4061, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [39/50], Train Loss: 0.4312, Test Loss: 0.4090, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [40/50], Train Loss: 0.4468, Test Loss: 0.3977, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [41/50], Train Loss: 0.4542, Test Loss: 0.3968, \n",
      "⚠️ No improvement. Patience counter = 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:00,551] Trial 2 finished with value: 0.3918066366630442 and parameters: {'num_hidden_layer': 4, 'neurons_per_hidden_layer': 192, 'lr': 3.947574760885219e-05, 'weight_decay': 1.8370594664823353e-05, 'dropout_rate': 0.3652866248086337, 'batch_size': 16, 'epochs': 50, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.3105880320072174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Train Loss: 0.4242, Test Loss: 0.4021, \n",
      "⚠️ No improvement. Patience counter = 5/5\n",
      "⏹ Early stopping at epoch 42 (best loss = 0.3918)\n",
      "Epoch [1/50], Train Loss: 1.0169, Test Loss: 0.8061, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.8061\n",
      "Epoch [2/50], Train Loss: 0.9418, Test Loss: 0.7649, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.7649\n",
      "Epoch [3/50], Train Loss: 0.9375, Test Loss: 0.7670, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [4/50], Train Loss: 0.8425, Test Loss: 0.7510, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.7510\n",
      "Epoch [5/50], Train Loss: 0.8504, Test Loss: 0.7355, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.7355\n",
      "Epoch [6/50], Train Loss: 0.8192, Test Loss: 0.7184, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.7184\n",
      "Epoch [7/50], Train Loss: 0.8135, Test Loss: 0.7109, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.7109\n",
      "Epoch [8/50], Train Loss: 0.7970, Test Loss: 0.6898, \n",
      "✅ Model improved. Saved at epoch 8 with Test Loss 0.6898\n",
      "Epoch [9/50], Train Loss: 0.8082, Test Loss: 0.6989, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [10/50], Train Loss: 0.7840, Test Loss: 0.6822, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.6822\n",
      "Epoch [11/50], Train Loss: 0.7970, Test Loss: 0.6825, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [12/50], Train Loss: 0.8053, Test Loss: 0.6697, \n",
      "✅ Model improved. Saved at epoch 12 with Test Loss 0.6697\n",
      "Epoch [13/50], Train Loss: 0.7437, Test Loss: 0.6671, \n",
      "✅ Model improved. Saved at epoch 13 with Test Loss 0.6671\n",
      "Epoch [14/50], Train Loss: 0.7908, Test Loss: 0.6638, \n",
      "✅ Model improved. Saved at epoch 14 with Test Loss 0.6638\n",
      "Epoch [15/50], Train Loss: 0.7768, Test Loss: 0.6657, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [16/50], Train Loss: 0.7877, Test Loss: 0.6534, \n",
      "✅ Model improved. Saved at epoch 16 with Test Loss 0.6534\n",
      "Epoch [17/50], Train Loss: 0.7815, Test Loss: 0.6540, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [18/50], Train Loss: 0.7324, Test Loss: 0.6540, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [19/50], Train Loss: 0.7381, Test Loss: 0.6514, \n",
      "✅ Model improved. Saved at epoch 19 with Test Loss 0.6514\n",
      "Epoch [20/50], Train Loss: 0.7464, Test Loss: 0.6596, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [21/50], Train Loss: 0.7461, Test Loss: 0.6461, \n",
      "✅ Model improved. Saved at epoch 21 with Test Loss 0.6461\n",
      "Epoch [22/50], Train Loss: 0.7297, Test Loss: 0.6474, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [23/50], Train Loss: 0.7125, Test Loss: 0.6468, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [24/50], Train Loss: 0.7228, Test Loss: 0.6440, \n",
      "✅ Model improved. Saved at epoch 24 with Test Loss 0.6440\n",
      "Epoch [25/50], Train Loss: 0.7489, Test Loss: 0.6422, \n",
      "✅ Model improved. Saved at epoch 25 with Test Loss 0.6422\n",
      "Epoch [26/50], Train Loss: 0.7275, Test Loss: 0.6337, \n",
      "✅ Model improved. Saved at epoch 26 with Test Loss 0.6337\n",
      "Epoch [27/50], Train Loss: 0.7466, Test Loss: 0.6218, \n",
      "✅ Model improved. Saved at epoch 27 with Test Loss 0.6218\n",
      "Epoch [28/50], Train Loss: 0.7398, Test Loss: 0.6542, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [29/50], Train Loss: 0.7314, Test Loss: 0.6487, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [30/50], Train Loss: 0.7139, Test Loss: 0.6386, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [31/50], Train Loss: 0.6861, Test Loss: 0.6248, \n",
      "⚠️ No improvement. Patience counter = 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:12,506] Trial 3 finished with value: 0.6218139108489541 and parameters: {'num_hidden_layer': 5, 'neurons_per_hidden_layer': 192, 'lr': 0.00048365982856225613, 'weight_decay': 0.0013281448882618058, 'dropout_rate': 0.3544370386760839, 'batch_size': 16, 'epochs': 50, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.3105880320072174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Train Loss: 0.6876, Test Loss: 0.6347, \n",
      "⚠️ No improvement. Patience counter = 5/5\n",
      "⏹ Early stopping at epoch 32 (best loss = 0.6218)\n",
      "Epoch [1/20], Train Loss: 1.0065, Test Loss: 0.6868, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.6868\n",
      "Epoch [2/20], Train Loss: 0.9932, Test Loss: 0.6797, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.6797\n",
      "Epoch [3/20], Train Loss: 0.9887, Test Loss: 0.6852, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [4/20], Train Loss: 0.9463, Test Loss: 0.6863, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [5/20], Train Loss: 0.9634, Test Loss: 0.6712, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.6712\n",
      "Epoch [6/20], Train Loss: 0.9759, Test Loss: 0.6784, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [7/20], Train Loss: 0.9514, Test Loss: 0.6800, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [8/20], Train Loss: 0.9304, Test Loss: 0.6783, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [9/20], Train Loss: 0.9287, Test Loss: 0.6820, \n",
      "⚠️ No improvement. Patience counter = 4/5\n",
      "Epoch [10/20], Train Loss: 0.9441, Test Loss: 0.6696, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.6696\n",
      "Epoch [11/20], Train Loss: 0.9450, Test Loss: 0.6735, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [12/20], Train Loss: 0.8954, Test Loss: 0.6686, \n",
      "✅ Model improved. Saved at epoch 12 with Test Loss 0.6686\n",
      "Epoch [13/20], Train Loss: 0.9083, Test Loss: 0.6730, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [14/20], Train Loss: 0.8381, Test Loss: 0.6761, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [15/20], Train Loss: 0.8805, Test Loss: 0.6788, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [16/20], Train Loss: 0.8810, Test Loss: 0.6765, \n",
      "⚠️ No improvement. Patience counter = 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:18,807] Trial 4 finished with value: 0.6685849042499766 and parameters: {'num_hidden_layer': 5, 'neurons_per_hidden_layer': 64, 'lr': 0.00024427761505925276, 'weight_decay': 0.0039226875922196225, 'dropout_rate': 0.441269846129134, 'batch_size': 16, 'epochs': 20, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.3105880320072174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Train Loss: 0.8936, Test Loss: 0.6759, \n",
      "⚠️ No improvement. Patience counter = 5/5\n",
      "⏹ Early stopping at epoch 17 (best loss = 0.6686)\n",
      "Epoch [1/20], Train Loss: 0.5505, Test Loss: 0.3665, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.3665\n",
      "Epoch [2/20], Train Loss: 0.4364, Test Loss: 0.3445, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.3445\n",
      "Epoch [3/20], Train Loss: 0.3807, Test Loss: 0.3313, \n",
      "✅ Model improved. Saved at epoch 3 with Test Loss 0.3313\n",
      "Epoch [4/20], Train Loss: 0.3770, Test Loss: 0.3067, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.3067\n",
      "Epoch [5/20], Train Loss: 0.3647, Test Loss: 0.2958, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.2958\n",
      "Epoch [6/20], Train Loss: 0.3501, Test Loss: 0.3024, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [7/20], Train Loss: 0.3644, Test Loss: 0.2944, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.2944\n",
      "Epoch [8/20], Train Loss: 0.3342, Test Loss: 0.3107, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [9/20], Train Loss: 0.3453, Test Loss: 0.2852, \n",
      "✅ Model improved. Saved at epoch 9 with Test Loss 0.2852\n",
      "Epoch [10/20], Train Loss: 0.3499, Test Loss: 0.3081, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [11/20], Train Loss: 0.3442, Test Loss: 0.2854, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [12/20], Train Loss: 0.2985, Test Loss: 0.2671, \n",
      "✅ Model improved. Saved at epoch 12 with Test Loss 0.2671\n",
      "Epoch [13/20], Train Loss: 0.3403, Test Loss: 0.2774, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [14/20], Train Loss: 0.3422, Test Loss: 0.2647, \n",
      "✅ Model improved. Saved at epoch 14 with Test Loss 0.2647\n",
      "Epoch [15/20], Train Loss: 0.3389, Test Loss: 0.2659, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [16/20], Train Loss: 0.3321, Test Loss: 0.2621, \n",
      "✅ Model improved. Saved at epoch 16 with Test Loss 0.2621\n",
      "Epoch [17/20], Train Loss: 0.3168, Test Loss: 0.2617, \n",
      "✅ Model improved. Saved at epoch 17 with Test Loss 0.2617\n",
      "Epoch [18/20], Train Loss: 0.3292, Test Loss: 0.2860, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [19/20], Train Loss: 0.3174, Test Loss: 0.2643, \n",
      "⚠️ No improvement. Patience counter = 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:25,855] Trial 5 finished with value: 0.2616649962523404 and parameters: {'num_hidden_layer': 4, 'neurons_per_hidden_layer': 112, 'lr': 0.0016882815273173066, 'weight_decay': 2.861148895806235e-05, 'dropout_rate': 0.24655773427524139, 'batch_size': 16, 'epochs': 20, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.2616649962523404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Train Loss: 0.3249, Test Loss: 0.2716, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [1/50], Train Loss: 0.7918, Test Loss: 0.6919, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.6919\n",
      "Epoch [2/50], Train Loss: 0.7553, Test Loss: 0.6423, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.6423\n",
      "Epoch [3/50], Train Loss: 0.7373, Test Loss: 0.6439, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [4/50], Train Loss: 0.7284, Test Loss: 0.6307, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.6307\n",
      "Epoch [5/50], Train Loss: 0.6732, Test Loss: 0.6304, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.6304\n",
      "Epoch [6/50], Train Loss: 0.6637, Test Loss: 0.5878, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.5878\n",
      "Epoch [7/50], Train Loss: 0.6653, Test Loss: 0.5810, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.5810\n",
      "Epoch [8/50], Train Loss: 0.6617, Test Loss: 0.5635, \n",
      "✅ Model improved. Saved at epoch 8 with Test Loss 0.5635\n",
      "Epoch [9/50], Train Loss: 0.6148, Test Loss: 0.5692, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [10/50], Train Loss: 0.6177, Test Loss: 0.5510, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.5510\n",
      "Epoch [11/50], Train Loss: 0.6125, Test Loss: 0.5474, \n",
      "✅ Model improved. Saved at epoch 11 with Test Loss 0.5474\n",
      "Epoch [12/50], Train Loss: 0.5765, Test Loss: 0.5365, \n",
      "✅ Model improved. Saved at epoch 12 with Test Loss 0.5365\n",
      "Epoch [13/50], Train Loss: 0.5947, Test Loss: 0.5186, \n",
      "✅ Model improved. Saved at epoch 13 with Test Loss 0.5186\n",
      "Epoch [14/50], Train Loss: 0.5799, Test Loss: 0.5072, \n",
      "✅ Model improved. Saved at epoch 14 with Test Loss 0.5072\n",
      "Epoch [15/50], Train Loss: 0.5567, Test Loss: 0.5207, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [16/50], Train Loss: 0.5738, Test Loss: 0.5100, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [17/50], Train Loss: 0.5708, Test Loss: 0.4979, \n",
      "✅ Model improved. Saved at epoch 17 with Test Loss 0.4979\n",
      "Epoch [18/50], Train Loss: 0.5491, Test Loss: 0.5025, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [19/50], Train Loss: 0.5521, Test Loss: 0.4820, \n",
      "✅ Model improved. Saved at epoch 19 with Test Loss 0.4820\n",
      "Epoch [20/50], Train Loss: 0.5504, Test Loss: 0.4536, \n",
      "✅ Model improved. Saved at epoch 20 with Test Loss 0.4536\n",
      "Epoch [21/50], Train Loss: 0.5364, Test Loss: 0.4725, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [22/50], Train Loss: 0.5241, Test Loss: 0.4613, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [23/50], Train Loss: 0.5126, Test Loss: 0.4716, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [24/50], Train Loss: 0.5092, Test Loss: 0.4395, \n",
      "✅ Model improved. Saved at epoch 24 with Test Loss 0.4395\n",
      "Epoch [25/50], Train Loss: 0.5072, Test Loss: 0.4498, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [26/50], Train Loss: 0.4956, Test Loss: 0.4563, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [27/50], Train Loss: 0.5103, Test Loss: 0.4254, \n",
      "✅ Model improved. Saved at epoch 27 with Test Loss 0.4254\n",
      "Epoch [28/50], Train Loss: 0.4919, Test Loss: 0.4256, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [29/50], Train Loss: 0.4889, Test Loss: 0.4237, \n",
      "✅ Model improved. Saved at epoch 29 with Test Loss 0.4237\n",
      "Epoch [30/50], Train Loss: 0.5152, Test Loss: 0.4390, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [31/50], Train Loss: 0.4748, Test Loss: 0.4369, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [32/50], Train Loss: 0.4910, Test Loss: 0.4078, \n",
      "✅ Model improved. Saved at epoch 32 with Test Loss 0.4078\n",
      "Epoch [33/50], Train Loss: 0.4798, Test Loss: 0.4200, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [34/50], Train Loss: 0.4622, Test Loss: 0.4105, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [35/50], Train Loss: 0.4827, Test Loss: 0.4145, \n",
      "⚠️ No improvement. Patience counter = 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:28,573] Trial 6 finished with value: 0.40782698392868044 and parameters: {'num_hidden_layer': 2, 'neurons_per_hidden_layer': 16, 'lr': 0.0004219081870824761, 'weight_decay': 0.0010042579177842793, 'dropout_rate': 0.34692902158422056, 'batch_size': 64, 'epochs': 50, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.2616649962523404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Train Loss: 0.4895, Test Loss: 0.4230, \n",
      "⚠️ No improvement. Patience counter = 4/5\n",
      "Epoch [37/50], Train Loss: 0.4841, Test Loss: 0.4207, \n",
      "⚠️ No improvement. Patience counter = 5/5\n",
      "⏹ Early stopping at epoch 37 (best loss = 0.4078)\n",
      "Epoch [1/20], Train Loss: 0.7611, Test Loss: 0.6923, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.6923\n",
      "Epoch [2/20], Train Loss: 0.7557, Test Loss: 0.6669, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.6669\n",
      "Epoch [3/20], Train Loss: 0.7443, Test Loss: 0.6757, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [4/20], Train Loss: 0.7488, Test Loss: 0.6509, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.6509\n",
      "Epoch [5/20], Train Loss: 0.7271, Test Loss: 0.6603, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [6/20], Train Loss: 0.7424, Test Loss: 0.6361, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.6361\n",
      "Epoch [7/20], Train Loss: 0.7056, Test Loss: 0.6287, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.6287\n",
      "Epoch [8/20], Train Loss: 0.7140, Test Loss: 0.6355, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [9/20], Train Loss: 0.7118, Test Loss: 0.6219, \n",
      "✅ Model improved. Saved at epoch 9 with Test Loss 0.6219\n",
      "Epoch [10/20], Train Loss: 0.6892, Test Loss: 0.6096, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.6096\n",
      "Epoch [11/20], Train Loss: 0.6903, Test Loss: 0.6073, \n",
      "✅ Model improved. Saved at epoch 11 with Test Loss 0.6073\n",
      "Epoch [12/20], Train Loss: 0.6751, Test Loss: 0.6053, \n",
      "✅ Model improved. Saved at epoch 12 with Test Loss 0.6053\n",
      "Epoch [13/20], Train Loss: 0.6524, Test Loss: 0.5946, \n",
      "✅ Model improved. Saved at epoch 13 with Test Loss 0.5946\n",
      "Epoch [14/20], Train Loss: 0.6286, Test Loss: 0.5829, \n",
      "✅ Model improved. Saved at epoch 14 with Test Loss 0.5829\n",
      "Epoch [15/20], Train Loss: 0.6725, Test Loss: 0.5972, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [16/20], Train Loss: 0.6510, Test Loss: 0.5669, \n",
      "✅ Model improved. Saved at epoch 16 with Test Loss 0.5669\n",
      "Epoch [17/20], Train Loss: 0.6397, Test Loss: 0.5703, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [18/20], Train Loss: 0.6393, Test Loss: 0.5620, \n",
      "✅ Model improved. Saved at epoch 18 with Test Loss 0.5620\n",
      "Epoch [19/20], Train Loss: 0.6527, Test Loss: 0.5515, \n",
      "✅ Model improved. Saved at epoch 19 with Test Loss 0.5515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:30,674] Trial 7 finished with value: 0.5405612349510193 and parameters: {'num_hidden_layer': 4, 'neurons_per_hidden_layer': 144, 'lr': 3.760239587290328e-05, 'weight_decay': 0.0026092464883826355, 'dropout_rate': 0.36957578263888824, 'batch_size': 64, 'epochs': 20, 'optimizer': 'Adam'}. Best is trial 5 with value: 0.2616649962523404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Train Loss: 0.6215, Test Loss: 0.5406, \n",
      "✅ Model improved. Saved at epoch 20 with Test Loss 0.5406\n",
      "Epoch [1/30], Train Loss: 0.8222, Test Loss: 0.6448, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.6448\n",
      "Epoch [2/30], Train Loss: 0.6417, Test Loss: 0.5520, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.5520\n",
      "Epoch [3/30], Train Loss: 0.5892, Test Loss: 0.5024, \n",
      "✅ Model improved. Saved at epoch 3 with Test Loss 0.5024\n",
      "Epoch [4/30], Train Loss: 0.5571, Test Loss: 0.4819, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.4819\n",
      "Epoch [5/30], Train Loss: 0.5050, Test Loss: 0.4674, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.4674\n",
      "Epoch [6/30], Train Loss: 0.4855, Test Loss: 0.4612, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.4612\n",
      "Epoch [7/30], Train Loss: 0.4885, Test Loss: 0.4243, \n",
      "✅ Model improved. Saved at epoch 7 with Test Loss 0.4243\n",
      "Epoch [8/30], Train Loss: 0.4527, Test Loss: 0.4252, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [9/30], Train Loss: 0.4369, Test Loss: 0.4148, \n",
      "✅ Model improved. Saved at epoch 9 with Test Loss 0.4148\n",
      "Epoch [10/30], Train Loss: 0.4308, Test Loss: 0.3928, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.3928\n",
      "Epoch [11/30], Train Loss: 0.4399, Test Loss: 0.3989, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [12/30], Train Loss: 0.4185, Test Loss: 0.4136, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [13/30], Train Loss: 0.4201, Test Loss: 0.3879, \n",
      "✅ Model improved. Saved at epoch 13 with Test Loss 0.3879\n",
      "Epoch [14/30], Train Loss: 0.4213, Test Loss: 0.3906, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [15/30], Train Loss: 0.4084, Test Loss: 0.3867, \n",
      "✅ Model improved. Saved at epoch 15 with Test Loss 0.3867\n",
      "Epoch [16/30], Train Loss: 0.3848, Test Loss: 0.3684, \n",
      "✅ Model improved. Saved at epoch 16 with Test Loss 0.3684\n",
      "Epoch [17/30], Train Loss: 0.3942, Test Loss: 0.3760, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [18/30], Train Loss: 0.3848, Test Loss: 0.3880, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [19/30], Train Loss: 0.4001, Test Loss: 0.3768, \n",
      "⚠️ No improvement. Patience counter = 3/5\n",
      "Epoch [20/30], Train Loss: 0.3810, Test Loss: 0.3685, \n",
      "⚠️ No improvement. Patience counter = 4/5\n",
      "Epoch [21/30], Train Loss: 0.3975, Test Loss: 0.3612, \n",
      "✅ Model improved. Saved at epoch 21 with Test Loss 0.3612\n",
      "Epoch [22/30], Train Loss: 0.3877, Test Loss: 0.3578, \n",
      "✅ Model improved. Saved at epoch 22 with Test Loss 0.3578\n",
      "Epoch [23/30], Train Loss: 0.3956, Test Loss: 0.3652, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [24/30], Train Loss: 0.3852, Test Loss: 0.3578, \n",
      "⚠️ No improvement. Patience counter = 2/5\n",
      "Epoch [25/30], Train Loss: 0.3867, Test Loss: 0.3677, \n",
      "⚠️ No improvement. Patience counter = 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:34,495] Trial 8 finished with value: 0.3578377382622825 and parameters: {'num_hidden_layer': 2, 'neurons_per_hidden_layer': 128, 'lr': 7.067034723785334e-05, 'weight_decay': 0.0006234385237695559, 'dropout_rate': 0.23659996996402177, 'batch_size': 32, 'epochs': 30, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.2616649962523404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Train Loss: 0.3681, Test Loss: 0.3655, \n",
      "⚠️ No improvement. Patience counter = 4/5\n",
      "Epoch [27/30], Train Loss: 0.3711, Test Loss: 0.3619, \n",
      "⚠️ No improvement. Patience counter = 5/5\n",
      "⏹ Early stopping at epoch 27 (best loss = 0.3578)\n",
      "Epoch [1/10], Train Loss: 0.6777, Test Loss: 0.5454, \n",
      "✅ Model improved. Saved at epoch 1 with Test Loss 0.5454\n",
      "Epoch [2/10], Train Loss: 0.5690, Test Loss: 0.4926, \n",
      "✅ Model improved. Saved at epoch 2 with Test Loss 0.4926\n",
      "Epoch [3/10], Train Loss: 0.5398, Test Loss: 0.4575, \n",
      "✅ Model improved. Saved at epoch 3 with Test Loss 0.4575\n",
      "Epoch [4/10], Train Loss: 0.5038, Test Loss: 0.4504, \n",
      "✅ Model improved. Saved at epoch 4 with Test Loss 0.4504\n",
      "Epoch [5/10], Train Loss: 0.4830, Test Loss: 0.4412, \n",
      "✅ Model improved. Saved at epoch 5 with Test Loss 0.4412\n",
      "Epoch [6/10], Train Loss: 0.4584, Test Loss: 0.4144, \n",
      "✅ Model improved. Saved at epoch 6 with Test Loss 0.4144\n",
      "Epoch [7/10], Train Loss: 0.4396, Test Loss: 0.4238, \n",
      "⚠️ No improvement. Patience counter = 1/5\n",
      "Epoch [8/10], Train Loss: 0.4455, Test Loss: 0.4022, \n",
      "✅ Model improved. Saved at epoch 8 with Test Loss 0.4022\n",
      "Epoch [9/10], Train Loss: 0.4334, Test Loss: 0.3922, \n",
      "✅ Model improved. Saved at epoch 9 with Test Loss 0.3922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 23:44:37,068] Trial 9 finished with value: 0.38795893946114707 and parameters: {'num_hidden_layer': 2, 'neurons_per_hidden_layer': 224, 'lr': 2.610701454467616e-05, 'weight_decay': 0.0002930514024460913, 'dropout_rate': 0.192554600593433, 'batch_size': 16, 'epochs': 10, 'optimizer': 'RMSprop'}. Best is trial 5 with value: 0.2616649962523404.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.4373, Test Loss: 0.3880, \n",
      "✅ Model improved. Saved at epoch 10 with Test Loss 0.3880\n"
     ]
    }
   ],
   "source": [
    "#now we will run the study object.\n",
    "study.optimize(func=objective,n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5398debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Best Loss: 0.2616649962523404\n",
      "  Params:\n",
      "    num_hidden_layer: 4\n",
      "    neurons_per_hidden_layer: 112\n",
      "    lr: 0.0016882815273173066\n",
      "    weight_decay: 2.861148895806235e-05\n",
      "    dropout_rate: 0.24655773427524139\n",
      "    batch_size: 16\n",
      "    epochs: 20\n",
      "    optimizer: Adam\n"
     ]
    }
   ],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial   # single best trial\n",
    "print(\"  Best Loss:\", trial.value)\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999a170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ec75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b9ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f745c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
